{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Gazetteers to Extract Sets of Keywords from Free-Flowing Texts #\n",
    "\n",
    "For [The Programming Historian](https://programminghistorian.org/en/lessons/extracting-keywords) by Adam Crymble. Modifed to Python 3 and Jupyter Notebooks for Historical Informatics at University of Southern Denmark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson Goals ##\n",
    "\n",
    "If you have a copy of a text in electronic format stored on your computer, it is relatively easy to keyword search for a single term. Often you can do this by using the built-in search features in your favourite text editor. However, scholars are increasingly needing to find instances of many terms within a text or texts. For example, a scholar may want to use a gazetteer to extract all mentions of English placenames within a collection of texts so that those places can later be plotted on a map. Alternatively, they may want to extract all male given names, all pronouns, stop words, or any other set of words. Using those same built-in search features to achieve this more complex goal is time consuming and clunky. This lesson will teach you how to use Python to extract a set of keywords very quickly and systematically from a set of texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content ##\n",
    "\n",
    "The present tutorial will show users how to extract all mentions of English and Welsh county names from a series of 6,692 mini-biographies of individuals who began their studies at the University of Oxford during the reign of James I of England (1603-1625). These records were transcribed by British History Online, from the printed version of Alumni Oxonienses, 1500-1714. These biographies contain information about each graduate, which includes the date of their studies and the college(s) they attended. Often entries contain additional information when known, including date or birth and death, the name or occupation of their father, where they originated, and what they went on to do in later life. The biographies are a rich resource, providing reasonably comparable data about a large number of similar individuals (rich men who went to Oxford). The 6,692 entries have been pre-processed by the author and saved to a CSV file with one entry per row.\n",
    "\n",
    "In this tutorial, the dataset involves geographical keywords. Once extracted, these placenames could be geo-referenced to their place on the globe and then mapped using digital mapping. This might make it possible to discern which colleges attracted students from what parts of the country, or to determine if these patterns changed over time. For a practical tutorial on taking this next step, see the lesson by Fred Gibbs mentioned at the end of this lesson. Readers may also be interested in georeferencing in QGIS 2.0, also available from the Programming Historian.\n",
    "\n",
    "This approach is not limited to geographical keywords, however. It could also be used to extract given names, prepositions, food words, or any other set of terms defined by the user. This process could therefore be useful for someone seeking to isolate individual entries containing any of these keywords, or for someone looking to calculate the frequency of their keywords within a corpus of texts. This tutorial provides pathways into textual or geospatial analyses, rather than research answers in its own right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set: Mini-biographies from University of Oxford (1603-25) ##\n",
    "\n",
    "We us the pandas library to inspect our data. Pandas is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way toward this goal.\n",
    "\n",
    "pandas is well suited for many different kinds of data:\n",
    "   - Tabular data with heterogeneously-typed columns, as in an SQL table or Excel spreadsheet\n",
    "   - Ordered and unordered (not necessarily fixed-frequency) time series data.\n",
    "   - Arbitrary matrix data (homogeneously typed or heterogeneous) with row and column labels\n",
    "   - Any other form of observational / statistical data sets. The data actually need not be labeled at all to be  - placed into a pandas data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "#data_path = os.path.join(\"..\",\"data\",\"Alumni_Oxonienses_Jas1.csv\")\n",
    "data_path = os.path.join(\"..\",\"data\",\"Alumni_Oxonienses_Jas1_utf8.csv\")\n",
    "\n",
    "#data = pd.read_csv(data_path,encoding=\"latin-1\")\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# column (variable) names\n",
    "print(data.columns)\n",
    "\n",
    "# first 10 rows\n",
    "print(data.head(10))\n",
    "\n",
    "# last 10 rows\n",
    "print(data.tail(10))\n",
    "\n",
    "# first entry for variable \"Details\"\n",
    "print(data[\"Details\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your gazetteer ##\n",
    "\n",
    "In order to extract the relevant place names, we first have to decide what they are. We need a list of places, often called a *gazetteer*. Many of the place names mentioned in the records are shortforms, such as ‘Wilts’ instead of ‘Wiltshire’, or ‘Salop’ instead of ‘Shropshire’. Getting all of these variations may be tricky. For a start, let’s build a basic gazetteer of English counties.\n",
    "\n",
    "Create a text file called `gazetteer.txt` and using the entries listed on the Wikipedia page listed above, add each county to a new line on the text file. It should look something like this:\n",
    "\n",
    "```\n",
    "Bedfordshire\n",
    "Berkshire\n",
    "Buckinghamshire\n",
    "Cambridgeshire\n",
    "Cheshire\n",
    "Cornwall\n",
    "Cumberland\n",
    "Derbyshire\n",
    "Devon\n",
    "Dorset\n",
    "Durham\n",
    "Essex\n",
    "Gloucestershire\n",
    "Hampshire\n",
    "Herefordshire\n",
    "Hertfordshire\n",
    "Huntingdonshire\n",
    "Kent\n",
    "Lancashire\n",
    "Leicestershire\n",
    "Lincolnshire\n",
    "Middlesex\n",
    "Norfolk\n",
    "Northamptonshire\n",
    "Northumberland\n",
    "Nottinghamshire\n",
    "Oxfordshire\n",
    "Rutland\n",
    "Shropshire\n",
    "Somerset\n",
    "Staffordshire\n",
    "Suffolk\n",
    "Surrey\n",
    "Sussex\n",
    "Warwickshire\n",
    "Westmorland\n",
    "Wiltshire\n",
    "Worcestershire\n",
    "Yorkshire\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import and export `.txt` files ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create texts.txt file ###\n",
    "\n",
    "Here we create a flat file from one column in the spreadsheet, but you could also have copy-pasted data from other sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = data[\"Details\"].tolist()\n",
    "\n",
    "file_path = os.path.join(\"..\",\"data\",\"texts.txt\")\n",
    "with open(file_path, 'w') as f:\n",
    "    for text in texts:\n",
    "        f.write(\"{}\\n\".format(text))# \\n for new line, i.e., one line pr. entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load `gazetteer.txt` ###\n",
    "\n",
    "Extract keywords from `gazetteer.txt` and store in `list()` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the keywords\n",
    "file_path = os.path.join(\"..\",\"resources\",\"gazetteer.txt\")\n",
    "\n",
    "with open(file_path,\"r\") as fname:\n",
    "    keywords = fname.read().lower().split(\"\\n\")\n",
    "\n",
    "print(\"The gazetteer contains {} locations:\".format(len(keywords)))\n",
    "\n",
    "print(keywords)\n",
    "\n",
    "print(\"The gazetteer is stored in at {} object\".format(type(keywords)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load `texts.txt` ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the texts you want to search\n",
    "file_path = os.path.join(\"..\",\"data\",\"texts.txt\")\n",
    "with open(file_path,\"r\") as fname:\n",
    "    texts = fname.read().lower().split(\"\\n\")\n",
    "\n",
    "for text in texts[:10]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing ##\n",
    "\n",
    "When matching strings, you have to make sure the punctuation doesn't get in the way. Technically, `London.` is a different string than `London` or `;London` because of the added punctuation. These three strings which all mean the same thing to us as human readers will be viewed by the computer as distinct entities. To solve that problem, the easiest thing to do is just to remove all of the punctuation. You can do this with regular expressions, and Doug Knox and Laura Turner O’Hara have provided great introductions at Programming Historian for doing so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenize and remove unwanted punctuation ###\n",
    "To keep things simple, this program will just replace the most common types of punctuation with nothing instead (effectively deleting punctuation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    #for each text:\n",
    "    tokens = text.split()\n",
    "    for token in tokens:\n",
    "        #remove punctuation that will interfere with matching\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\";\", \"\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String matching ##\n",
    "\n",
    "As the words from our text are already in a list called `tokens`, and all of our keywords are in a list called `keywords`, all we have to do now is check our texts for the keywords.\n",
    "\n",
    "First, we need somewhere to store details of any matches we have. Immediately after the `for text in texts:` line, at one level of indentation, add the following two lines of code:\n",
    "\n",
    "```\n",
    "matches = 0\n",
    "stored_matches = list()\n",
    "```\n",
    "\n",
    "Indentation is important in Python. The above two lines should be indented one tab deeper than the for loop above it. That means the code is to run every time the for loop runs - it is part of the loop.\n",
    "\n",
    "The `stored_matches` variable is a blank list, where we can store our matching keywords. The `matches` variable is known as a `flag`, which we will use in the next step when we start printing the output.\n",
    "\n",
    "To do the actual matching, add the following lines of code to the bottom of your program, again minding the indentation (2 levels from the left margin), making sure you save:\n",
    "\n",
    "```\n",
    "        #if a keyword match is found, store the result.\n",
    "        if token in keywords:\n",
    "            if token in stored_matches:\n",
    "                continue\n",
    "            else:\n",
    "                stored_matches.append(token)\n",
    "            matches += 1\n",
    "    print(matches)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    matches = 0\n",
    "    stored_matches = list()\n",
    "    #for each text:\n",
    "    tokens = text.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        #remove punctuation that will interfere with matching\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\";\", \"\")\n",
    "        \n",
    "        if token in keywords:\n",
    "            \n",
    "            if token in stored_matches:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                stored_matches.append(token)\n",
    "            \n",
    "            matches += 1\n",
    "            \n",
    "    print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output results ##\n",
    "\n",
    "If you have got to this stage, then your Python script is already finding the matching keywords from your gazetteer. All we need to do now is print them out to the command output pane in a format that’s easy to work with.\n",
    "\n",
    "Add the following lines to your program, minding the indentation as always:\n",
    "\n",
    "```\n",
    "    #if there is a stored result, print it out\n",
    "    if matches == 0:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        match_string = \"\"\n",
    "        for matches in stored_matches:\n",
    "            match_string = match_string + matches + \"\\t\"\n",
    "        \n",
    "        print(match_string)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    matches = 0\n",
    "    stored_matches = list()\n",
    "    #for each text:\n",
    "    tokens = text.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        #remove punctuation that will interfere with matching\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\";\", \"\")\n",
    "        \n",
    "        if token in keywords:\n",
    "            \n",
    "            if token in stored_matches:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                stored_matches.append(token)\n",
    "            \n",
    "            matches += 1\n",
    "    if matches == 0:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        match_string = \"\"\n",
    "        for matches in stored_matches:\n",
    "            match_string = match_string + matches + \"\\t\"\n",
    "            \n",
    "        print(match_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output matches to file ##\n",
    "Finalize script by appending each match to `.txt` file\n",
    "\n",
    "```\n",
    "    f = open('output.txt', 'a')\n",
    "    f.write(matchString)\n",
    "    f.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"..\",\"data\",\"output.txt\")\n",
    "\n",
    "for text in texts:\n",
    "    matches = 0\n",
    "    stored_matches = list()\n",
    "    #for each text:\n",
    "    tokens = text.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        #remove punctuation that will interfere with matching\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\";\", \"\")\n",
    "        \n",
    "        if token in keywords:\n",
    "            \n",
    "            if token in stored_matches:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                stored_matches.append(token)\n",
    "            \n",
    "            matches += 1\n",
    "    if matches == 0:\n",
    "        print(\"\")\n",
    "    else:\n",
    "        match_string = \"\"\n",
    "        for matches in stored_matches:\n",
    "            match_string = match_string + matches + \"\\t\"\n",
    "            \n",
    "        print(match_string)\n",
    "        \n",
    "        with open(file_path,\"a\") as fname:\n",
    "            fname.write(match_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `a` instead of the `r` we used earlier. This 'appends' the text to the file called `output.txt`, which will be saved in your in the directory on the file path in `file_path`. You will have to take care, because running the program several times will continue to append all of the outputs to this file, creating a very long file. We can solve this by 'appending' the matches in `match_string` to a list object `output` and exporting the list to a `output.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(\"..\",\"data\",\"output.txt\")\n",
    "output = list()\n",
    "\n",
    "for text in texts:\n",
    "    matches = 0\n",
    "    stored_matches = list()\n",
    "    #for each text:\n",
    "    tokens = text.split()\n",
    "    \n",
    "    for token in tokens:\n",
    "        #remove punctuation that will interfere with matching\n",
    "        token = token.replace(\",\", \"\")\n",
    "        token = token.replace(\".\", \"\")\n",
    "        token = token.replace(\";\", \"\")\n",
    "        \n",
    "        if token in keywords:\n",
    "            \n",
    "            if token in stored_matches:\n",
    "                continue\n",
    "                \n",
    "            else:\n",
    "                stored_matches.append(token)\n",
    "            \n",
    "            matches += 1\n",
    "    if matches == 0:\n",
    "        continue\n",
    "    else:\n",
    "        match_string = \"\"\n",
    "        for matches in stored_matches:\n",
    "            match_string = match_string + matches\n",
    "            \n",
    "        output.append(match_string)\n",
    "\n",
    "print(output)\n",
    "\n",
    "with open(file_path, 'w') as fname:\n",
    "    for match_string in output:\n",
    "        fname.write(\"{}\\n\".format(match_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
